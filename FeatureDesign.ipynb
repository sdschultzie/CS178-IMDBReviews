{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3cac8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you've\", 'off', 'will', 'now', 'she', 'does', \"shan't\", 've', 't', \"couldn't\", 'this', 'under', \"needn't\", 'more', 'they', 'while', 'wouldn', 'their', 'mustn', 'hadn', 'hers', 'out', 'too', 'had', 'as', 'each', 'those', 'me', \"weren't\", 'these', 'doing', 'needn', 'whom', 'with', 'are', 'you', 'don', \"you'll\", \"doesn't\", 'mightn', 'have', \"mustn't\", 'by', 'most', 'such', 'after', 'aren', \"haven't\", 'further', \"aren't\", 'be', \"isn't\", 'been', 'am', 'how', 'shouldn', 'above', 'y', 'do', 'themselves', 'and', 'about', 'but', 'shan', 'himself', 'into', 'or', 'yourselves', 'here', 'them', 'before', 'yourself', 'so', 'hasn', 'him', 'in', 'it', 'our', 'its', 'ourselves', 'ours', 'my', 'same', 's', 'll', \"hasn't\", 'the', 'until', 'all', 'through', 'a', 'having', 'his', \"didn't\", 'itself', \"that'll\", 'haven', 'i', 'myself', \"mightn't\", 'there', 'only', 'isn', \"hadn't\", 'doesn', 'can', \"you're\", 'did', 'herself', 're', 'if', 'once', \"wouldn't\", \"won't\", 'up', 'were', 'her', 'ain', 'against', 'both', \"don't\", 'just', 'down', 'then', 'is', 'for', 'again', 'not', 'nor', 'o', 'that', 'no', 'yours', 'being', 'from', 'why', 'which', 'your', \"shouldn't\", 'than', 'd', 'between', 'below', 'some', 'other', 'should', 'theirs', 'm', 'over', 'of', 'where', 'didn', \"wasn't\", 'was', 'he', 'very', 'we', 'any', 'weren', 'an', 'who', \"should've\", 'because', 'during', \"she's\", 'few', 'when', \"it's\", 'to', \"you'd\", 'what', 'own', 'wasn', 'won', 'couldn', 'on', 'at', 'has', 'ma'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.metrics import zero_one_loss, accuracy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import scipy\n",
    "\n",
    "import sklearn.tree as tree\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Fix the random seed for reproducibility\n",
    "# !! Important !! : do not change this\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "DATA_PATH = Path('data/original-dataset')\n",
    "VOCAB_PATH = DATA_PATH.joinpath('imdb.vocab')\n",
    "TRAIN_BOW_PATH = DATA_PATH.joinpath('train/labeledBow.feat')\n",
    "TEST_BOW_PATH = DATA_PATH.joinpath('test/labeledBow.feat')\n",
    "TRAIN_FILES = DATA_PATH.glob('train/[pn][oe][sg]/*.txt')\n",
    "TEST_FILES = DATA_PATH.glob('test/[pn][oe][sg]/*.txt')\n",
    "COLORMAP = 'seismic'\n",
    "\n",
    "# data processing constants\n",
    "MAX_DF = 0.4\n",
    "MIN_DF = 2\n",
    "PCA_COMPONENTS = 100\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english')))\n",
    "\n",
    "# just tf-idf score (row normalized)\n",
    "# just tf-idf score without stop words\n",
    "# tf-idf score with PCA\n",
    "# tf-idf score with PCA without stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "727b0bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "TRAIN_FILES = list(TRAIN_FILES)\n",
    "TEST_FILES = list(TEST_FILES)\n",
    "\n",
    "print(len(TRAIN_FILES))\n",
    "print(len(TEST_FILES))\n",
    "\n",
    "def y_label(review_path):\n",
    "    rating = int(review_path.stem.split('_')[1])\n",
    "    return 1 if rating > 5 else -1\n",
    "\n",
    "ytr = np.array([ y_label(path) for path in TRAIN_FILES])\n",
    "yte = np.array([ y_label(path) for path in TEST_FILES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3bd2957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Number of terms: 74849\n",
      "Top appearing words:\n",
      "[(1.008314135652267, 'the'), (1.0340105187711748, 'and'), (1.0522600386726457, 'of'), (1.0629372013721339, 'to'), (1.09919757073324, 'this'), (1.10865023392407, 'is'), (1.115002017669703, 'it'), (1.1260568397835082, 'in'), (1.2070149693526475, 'that'), (1.329544566505641, 'but')]\n"
     ]
    }
   ],
   "source": [
    "# Transforms review text into a vector of normalized tf-idf scores where each column corresponds to a term\n",
    "\n",
    "print('starting')\n",
    "tfidf_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(input='filename', norm='l2'))\n",
    "]).fit(TRAIN_FILES)\n",
    "\n",
    "sorted_terms = sorted((tfidf_pipe['tfidf'].idf_[index], term) for term, index in tfidf_pipe['tfidf'].vocabulary_.items())\n",
    "print(f\"Number of terms: {len(tfidf_pipe['tfidf'].vocabulary_)}\")\n",
    "print(f\"Top appearing words:\\n{sorted_terms[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc217205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "starting\n",
      "Number of terms: 44756\n",
      "Top appearing words:\n",
      "[(1.9223488033997396, 'what'), (1.9522688378798054, 'some'), (1.9527872680847915, 'good'), (1.9547597576939584, 'can'), (2.0142743754043027, 'more'), (2.0179205868358094, 'when'), (2.051577880482202, 'time'), (2.0587010723659036, 'up'), (2.0588163793709313, 'very'), (2.088305995813987, 'even')]\n"
     ]
    }
   ],
   "source": [
    "# Transforms review text into a vector of normalized tf-idf scores where each column corresponds to a term\n",
    "# This vectorization does not include terms in > 50% of the documents or terms in only one document\n",
    "print(len(list(TRAIN_FILES)))\n",
    "print('starting')\n",
    "tfidf_stop_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(input='filename', norm='l2', stop_words='english', max_df=MAX_DF, min_df=MIN_DF))\n",
    "]).fit(TRAIN_FILES)\n",
    "\n",
    "sorted_terms = sorted((tfidf_stop_pipe['tfidf'].idf_[index], term) for term, index in tfidf_stop_pipe['tfidf'].vocabulary_.items())\n",
    "print(f\"Number of terms: {len(tfidf_stop_pipe['tfidf'].vocabulary_)}\")\n",
    "print(f\"Top appearing words:\\n{sorted_terms[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ae7cc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "[54.65891888 13.5102238  12.79904101  9.8554702   9.10612403  8.39122164\n",
      "  8.1808897   7.93647635  7.58407459  7.06695881  6.96318136  6.66223256\n",
      "  6.50845629  6.35585221  6.18841767  6.06035663  5.69538676  5.6170094\n",
      "  5.46927002  5.39525987  5.28998828  5.27702923  5.24512621  5.18554463\n",
      "  5.15021639  5.04610149  4.99621167  4.96877061  4.94125512  4.81736395\n",
      "  4.77980955  4.75037253  4.71513501  4.69388025  4.65940478  4.64448641\n",
      "  4.62677912  4.57991376  4.56781248  4.56191947  4.51491515  4.47765129\n",
      "  4.46981404  4.45275512  4.43885221  4.42746863  4.42213615  4.38177575\n",
      "  4.36924251  4.35579305  4.32072653  4.31720049  4.29497183  4.28964809\n",
      "  4.26886973  4.25987924  4.24263091  4.23401665  4.21492532  4.19712404\n",
      "  4.18197244  4.17705185  4.16299535  4.15088578  4.14209026  4.12840679\n",
      "  4.11422708  4.07980799  4.07349028  4.06864937  4.05402221  4.0379094\n",
      "  4.03302017  4.01043881  3.99915594  3.99017694  3.978159    3.96857289\n",
      "  3.96398616  3.95332369  3.94731316  3.9311007   3.92015296  3.91345062\n",
      "  3.90493896  3.89955223  3.89263066  3.88319782  3.86826529  3.86521019\n",
      "  3.84146252  3.83281451  3.82259109  3.80789485  3.80253149  3.79613576\n",
      "  3.79461998  3.78645452  3.78540795  3.77443368]\n",
      "[0.00898517 0.00820851 0.00737354 0.00436467 0.00373224 0.0031701\n",
      " 0.00301323 0.00283029 0.0025873  0.00224676 0.00218295 0.00199831\n",
      " 0.00190281 0.00181876 0.00172421 0.00165126 0.00146039 0.00142015\n",
      " 0.00134639 0.00131056 0.00125991 0.00125343 0.00123857 0.00121043\n",
      " 0.00119421 0.0011463  0.00112386 0.00111149 0.00109906 0.00104478\n",
      " 0.00102829 0.00101597 0.00100091 0.0009918  0.000977   0.00097119\n",
      " 0.00096374 0.00094438 0.00093939 0.00093695 0.00091776 0.00090261\n",
      " 0.00089947 0.00089267 0.0008871  0.00088255 0.00088044 0.00086442\n",
      " 0.00085945 0.00085418 0.00084052 0.00083914 0.0008305  0.00082845\n",
      " 0.00082021 0.000817   0.0008104  0.00080712 0.00079986 0.00079312\n",
      " 0.00078734 0.00078551 0.00078026 0.00077574 0.00077244 0.00076731\n",
      " 0.00076209 0.00074936 0.00074706 0.00074528 0.00073994 0.00073405\n",
      " 0.0007323  0.00072407 0.00072006 0.00071681 0.00071252 0.00070909\n",
      " 0.00070743 0.00070364 0.00070148 0.00069574 0.00069188 0.00068943\n",
      " 0.0006865  0.00068464 0.0006822  0.00067889 0.0006737  0.00067262\n",
      " 0.00066439 0.00066136 0.00065788 0.00065283 0.000651   0.00064877\n",
      " 0.00064829 0.0006455  0.00064515 0.00064141]\n"
     ]
    }
   ],
   "source": [
    "# Transforms review text into a vector of the 100 largest principal components of the tf-idf vector\n",
    "\n",
    "print('starting')\n",
    "tfidf_pca_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(input='filename', norm='l2')),\n",
    "    ('svd', TruncatedSVD(n_components=PCA_COMPONENTS, algorithm='arpack')),\n",
    "    ('norm', Normalizer(norm='l2'))\n",
    "]).fit(TRAIN_FILES)\n",
    "\n",
    "\n",
    "print(tfidf_pca_pipe['svd'].singular_values_)\n",
    "print(tfidf_pca_pipe['svd'].explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70136ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "[28.08140773 10.06980569  8.10585976  7.52292096  7.07218213  6.79512768\n",
      "  6.28819711  6.26508273  6.04715284  5.81593504  5.71226509  5.60233955\n",
      "  5.50898792  5.44313683  5.34742121  5.32644107  5.26528791  5.24908213\n",
      "  5.20689789  5.15210224  5.07774344  5.03429671  4.97792161  4.92292713\n",
      "  4.9051308   4.86079953  4.8371571   4.7972661   4.77866402  4.74200835\n",
      "  4.73912072  4.7202972   4.70475769  4.69095278  4.65631607  4.64423152\n",
      "  4.61701328  4.60986515  4.57378701  4.55368827  4.54016578  4.51863127\n",
      "  4.50871852  4.48257625  4.47255448  4.45271639  4.44050295  4.41090566\n",
      "  4.40860529  4.39856359  4.38209627  4.37323762  4.35652325  4.34224017\n",
      "  4.31376637  4.29130353  4.28567114  4.27521185  4.26966697  4.25191315\n",
      "  4.23056591  4.22973425  4.22013324  4.20600713  4.20106264  4.18666918\n",
      "  4.17999375  4.17748505  4.15663992  4.14248832  4.12851382  4.12257462\n",
      "  4.10434417  4.09856888  4.09463173  4.08851844  4.07587692  4.05788589\n",
      "  4.05141607  4.04356284  4.03309064  4.02965351  4.01243472  4.00763817\n",
      "  3.99396826  3.98752977  3.98246215  3.97854365  3.9692086   3.9648448\n",
      "  3.96146147  3.95284993  3.94712577  3.92958748  3.92593673  3.92235438\n",
      "  3.91817187  3.90207286  3.89638764  3.89194947]\n",
      "[0.0029732  0.00417569 0.00268611 0.00232901 0.00205924 0.00190152\n",
      " 0.00161819 0.00161587 0.00150594 0.00139065 0.0013437  0.0012922\n",
      " 0.00124978 0.00121992 0.00117745 0.00116779 0.0011414  0.00113463\n",
      " 0.00111643 0.0010931  0.00106179 0.00104372 0.00102026 0.00099805\n",
      " 0.00099085 0.00097301 0.00096343 0.0009474  0.00094024 0.00092568\n",
      " 0.00092491 0.00091755 0.00091155 0.00090613 0.00089284 0.00088825\n",
      " 0.00087779 0.00087502 0.00086149 0.00085389 0.00084879 0.00084075\n",
      " 0.00083703 0.00082736 0.00082376 0.00081638 0.00081192 0.00080124\n",
      " 0.00080039 0.00079629 0.00079079 0.00078759 0.00078144 0.00077647\n",
      " 0.00076632 0.00075828 0.00075634 0.00075262 0.00075073 0.00074451\n",
      " 0.00073704 0.00073677 0.00073332 0.00072852 0.00072682 0.00072183\n",
      " 0.00071952 0.00071868 0.00071153 0.00070669 0.00070192 0.00069936\n",
      " 0.00069373 0.00069172 0.00069046 0.00068828 0.00068414 0.00067811\n",
      " 0.00067596 0.00067318 0.00066985 0.00066862 0.00066301 0.00066135\n",
      " 0.00065688 0.00065481 0.00065314 0.00065186 0.00064877 0.00064736\n",
      " 0.00064626 0.00064333 0.00064158 0.00063581 0.00063463 0.00063356\n",
      " 0.00063223 0.00062704 0.00062519 0.0006238 ]\n"
     ]
    }
   ],
   "source": [
    "# Transforms review text into a vector of the 100 largest principal components of the tf-idf vector\n",
    "# This vectorization does not include terms in > 50% of the documents or terms in only one document\n",
    "\n",
    "print('starting')\n",
    "tfidf_pca_stop_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(input='filename', norm='l2', max_df=MAX_DF, min_df=MIN_DF)),\n",
    "    ('svd', TruncatedSVD(n_components=PCA_COMPONENTS, algorithm='arpack')),\n",
    "    ('norm', Normalizer(norm='l2'))\n",
    "]).fit(TRAIN_FILES)\n",
    "\n",
    "\n",
    "print(tfidf_pca_stop_pipe['svd'].singular_values_)\n",
    "print(tfidf_pca_stop_pipe['svd'].explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62cb7e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Learner on tfidf features\n",
      "finished transform\n",
      "finished fitting\n",
      "train acc: 0.93328\n",
      "test acc : 0.88316\n",
      "Training Learner on tfidf stop features\n",
      "finished transform\n",
      "finished fitting\n",
      "train acc: 0.93488\n",
      "test acc : 0.88472\n",
      "Training Learner on tfidf PCA(100) features\n",
      "finished transform\n",
      "finished fitting\n",
      "train acc: 0.85548\n",
      "test acc : 0.85192\n",
      "Training Learner on tfidf stop PCA(100) features\n",
      "finished transform\n",
      "finished fitting\n",
      "train acc: 0.86088\n",
      "test acc : 0.85824\n"
     ]
    }
   ],
   "source": [
    "feature_models = [\n",
    "    ('tfidf', tfidf_pipe),\n",
    "    ('tfidf stop', tfidf_stop_pipe),\n",
    "    ('tfidf PCA(100)', tfidf_pca_pipe),\n",
    "    ('tfidf stop PCA(100)', tfidf_pca_stop_pipe)\n",
    "]\n",
    "\n",
    "for fm_name, fm_pipe in feature_models:\n",
    "    print(f'Training Learner on {fm_name} features')\n",
    "    Xtr_transform = fm_pipe.transform(TRAIN_FILES)\n",
    "    Xte_transform = fm_pipe.transform(TEST_FILES)\n",
    "    print('finished transform')\n",
    "    \n",
    "    learner = LogisticRegression()\n",
    "    learner.fit(Xtr_transform, ytr)\n",
    "    print('finished fitting')\n",
    "    \n",
    "    print(f'train acc: {learner.score(Xtr_transform, ytr)}')\n",
    "    print(f'test acc : {learner.score(Xte_transform, yte)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca488e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbedced0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs178",
   "language": "python",
   "name": "cs178"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
