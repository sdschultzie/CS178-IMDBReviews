{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5f01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, csv, time\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import datasets\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn import svm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk, re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7933972",
   "metadata": {},
   "source": [
    "CSV File Creation Blocks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "becfbc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train/pos\\10327_7.txt\n",
      "./train/pos\\11351_9.txt\n",
      "./train/pos\\11668_7.txt\n",
      "./train/pos\\12467_7.txt\n",
      "./train/pos\\1990_10.txt\n",
      "./train/pos\\2362_9.txt\n",
      "./train/pos\\2538_10.txt\n",
      "./train/pos\\4942_7.txt\n",
      "./train/pos\\5343_8.txt\n",
      "./train/pos\\8263_9.txt\n",
      "./train/pos\\9107_7.txt\n",
      "./train/neg\\11325_4.txt\n",
      "./train/neg\\12391_4.txt\n",
      "./train/neg\\4526_4.txt\n",
      "./train/neg\\6122_2.txt\n",
      "./train/neg\\7714_1.txt\n"
     ]
    }
   ],
   "source": [
    "# training csv file creation\n",
    "filename = \"training.csv\"\n",
    "\n",
    "with open(filename,\"w\",newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile,delimiter=',')\n",
    "    csvwriter.writerow([\"Text\",\"Sentiment\"]) \n",
    "    \n",
    "    for root, dirs, files in os.walk(\"./train/pos\", topdown=False):\n",
    "    #print(len(files))\n",
    "        for name in files:\n",
    "            file1 = open(os.path.join(root, name), \"r\")\n",
    "            #print(os.path.join(root, name))\n",
    "            try:\n",
    "                csvwriter.writerow([file1.read(),\"positive\"]) \n",
    "            except:\n",
    "                print(os.path.join(root, name))\n",
    "            file1.close()\n",
    "            \n",
    "    for root, dirs, files in os.walk(\"./train/neg\", topdown=False):\n",
    "    #print(len(files))\n",
    "        for name in files:\n",
    "            file1 = open(os.path.join(root, name), \"r\")\n",
    "            #print(os.path.join(root, name))\n",
    "            try:\n",
    "                csvwriter.writerow([file1.read(),\"negative\"]) \n",
    "            except:\n",
    "                print(os.path.join(root, name))\n",
    "            file1.close()\n",
    "    '''for root, dirs, files in os.walk(\"./train/unsup\", topdown=False):\n",
    "    #print(len(files))\n",
    "        for name in files:\n",
    "            file1 = open(os.path.join(root, name), \"r\")\n",
    "            #print(os.path.join(root, name))\n",
    "            try:\n",
    "                csvwriter.writerow([file1.read(),-1]) \n",
    "            except:\n",
    "                print(os.path.join(root, name))\n",
    "            file1.close()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a0076cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test/pos\\10267_7.txt\n",
      "./test/pos\\10923_7.txt\n",
      "./test/pos\\12164_8.txt\n",
      "./test/pos\\1905_8.txt\n",
      "./test/pos\\2464_10.txt\n",
      "./test/pos\\5210_10.txt\n",
      "./test/pos\\5281_10.txt\n",
      "./test/neg\\11173_2.txt\n",
      "./test/neg\\3696_4.txt\n",
      "./test/neg\\4414_1.txt\n",
      "./test/neg\\688_4.txt\n",
      "./test/neg\\6970_1.txt\n",
      "./test/neg\\6973_1.txt\n",
      "./test/neg\\8467_1.txt\n",
      "./test/neg\\9347_2.txt\n"
     ]
    }
   ],
   "source": [
    "# test csv file creation\n",
    "filename = \"labeled_and_unlabeled_dataset_test.csv\"\n",
    "\n",
    "with open(filename,\"w\",newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile,delimiter=',')\n",
    "    csvwriter.writerow([\"Text\",\"Sentiment\"]) \n",
    "    \n",
    "    for root, dirs, files in os.walk(\"./test/pos\", topdown=False):\n",
    "    #print(len(files))\n",
    "        for name in files:\n",
    "            file1 = open(os.path.join(root, name), \"r\")\n",
    "            #print(os.path.join(root, name))\n",
    "            try:\n",
    "                csvwriter.writerow([file1.read(),\"positive\"]) \n",
    "            except:\n",
    "                print(os.path.join(root, name))\n",
    "            file1.close()\n",
    "            \n",
    "    for root, dirs, files in os.walk(\"./test/neg\", topdown=False):\n",
    "    #print(len(files))\n",
    "        for name in files:\n",
    "            file1 = open(os.path.join(root, name), \"r\")\n",
    "            #print(os.path.join(root, name))\n",
    "            try:\n",
    "                csvwriter.writerow([file1.read(),\"negative\"]) \n",
    "            except:\n",
    "                print(os.path.join(root, name))\n",
    "            file1.close()\n",
    "            \n",
    "    for root, dirs, files in os.walk(\"./test/unsup\", topdown=False):\n",
    "    #print(len(files))\n",
    "        for name in files:\n",
    "            file1 = open(os.path.join(root, name), \"r\")\n",
    "            #print(os.path.join(root, name))\n",
    "            try:\n",
    "                csvwriter.writerow([file1.read(),-1]) \n",
    "            except:\n",
    "                print(os.path.join(root, name))\n",
    "            file1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb1994d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train/pos\\10327_7.txt\n",
      "./train/pos\\11351_9.txt\n",
      "./train/pos\\11668_7.txt\n",
      "./train/pos\\12467_7.txt\n",
      "./train/pos\\1990_10.txt\n",
      "./train/pos\\2362_9.txt\n",
      "./train/pos\\2538_10.txt\n",
      "./train/pos\\4942_7.txt\n",
      "./train/pos\\5343_8.txt\n",
      "./train/pos\\8263_9.txt\n",
      "./train/pos\\9107_7.txt\n",
      "./train/neg\\11325_4.txt\n",
      "./train/neg\\12391_4.txt\n",
      "./train/neg\\4526_4.txt\n",
      "./train/neg\\6122_2.txt\n",
      "./train/neg\\7714_1.txt\n"
     ]
    }
   ],
   "source": [
    "# training csv file creation with val reviews\n",
    "filename = \"training_vals.csv\"\n",
    "\n",
    "with open(filename,\"w\",newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile,delimiter=',')\n",
    "    csvwriter.writerow([\"Text\",\"Sentiment\"]) \n",
    "    \n",
    "    for root, dirs, files in os.walk(\"./train/pos\", topdown=False):\n",
    "    #print(len(files))\n",
    "        for name in files:\n",
    "            file1 = open(os.path.join(root, name), \"r\")\n",
    "            #print(os.path.join(root, name))\n",
    "            try:\n",
    "                csvwriter.writerow([file1.read(),name.split(\"_\")[1].split(\".\")[0]]) \n",
    "            except:\n",
    "                print(os.path.join(root, name))\n",
    "            file1.close()\n",
    "            \n",
    "    for root, dirs, files in os.walk(\"./train/neg\", topdown=False):\n",
    "    #print(len(files))\n",
    "        for name in files:\n",
    "            file1 = open(os.path.join(root, name), \"r\")\n",
    "            #print(os.path.join(root, name))\n",
    "            try:\n",
    "                csvwriter.writerow([file1.read(),name.split(\"_\")[1].split(\".\")[0]]) \n",
    "            except:\n",
    "                print(os.path.join(root, name))\n",
    "            file1.close()\n",
    "    '''for root, dirs, files in os.walk(\"./train/unsup\", topdown=False):\n",
    "    #print(len(files))\n",
    "        for name in files:\n",
    "            file1 = open(os.path.join(root, name), \"r\")\n",
    "            #print(os.path.join(root, name))\n",
    "            try:\n",
    "                csvwriter.writerow([file1.read(),-1]) \n",
    "            except:\n",
    "                print(os.path.join(root, name))\n",
    "            file1.close()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4dca570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test/pos\\10267_7.txt\n",
      "./test/pos\\10923_7.txt\n",
      "./test/pos\\12164_8.txt\n",
      "./test/pos\\1905_8.txt\n",
      "./test/pos\\2464_10.txt\n",
      "./test/pos\\5210_10.txt\n",
      "./test/pos\\5281_10.txt\n",
      "./test/neg\\11173_2.txt\n",
      "./test/neg\\3696_4.txt\n",
      "./test/neg\\4414_1.txt\n",
      "./test/neg\\688_4.txt\n",
      "./test/neg\\6970_1.txt\n",
      "./test/neg\\6973_1.txt\n",
      "./test/neg\\8467_1.txt\n",
      "./test/neg\\9347_2.txt\n"
     ]
    }
   ],
   "source": [
    "# test csv file creation with vals\n",
    "filename = \"test_vals.csv\"\n",
    "\n",
    "with open(filename,\"w\",newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile,delimiter=',')\n",
    "    csvwriter.writerow([\"Text\",\"Sentiment\"]) \n",
    "    \n",
    "    for root, dirs, files in os.walk(\"./test/pos\", topdown=False):\n",
    "    #print(len(files))\n",
    "        for name in files:\n",
    "            file1 = open(os.path.join(root, name), \"r\")\n",
    "            #print(os.path.join(root, name))\n",
    "            try:\n",
    "                csvwriter.writerow([file1.read(),name.split(\"_\")[1].split(\".\")[0]]) \n",
    "            except:\n",
    "                print(os.path.join(root, name))\n",
    "            file1.close()\n",
    "            \n",
    "    for root, dirs, files in os.walk(\"./test/neg\", topdown=False):\n",
    "    #print(len(files))\n",
    "        for name in files:\n",
    "            file1 = open(os.path.join(root, name), \"r\")\n",
    "            #print(os.path.join(root, name))\n",
    "            try:\n",
    "                csvwriter.writerow([file1.read(),name.split(\"_\")[1].split(\".\")[0]]) \n",
    "            except:\n",
    "                print(os.path.join(root, name))\n",
    "            file1.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e914df",
   "metadata": {},
   "source": [
    "Semi Supervised SGD Training Positive/Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad257d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_unlabeled=pd.read_csv(\"training_unlabeled.csv\")\n",
    "data_train = pd.read_csv(\"training.csv\")\n",
    "data_test = pd.read_csv(\"labeled_and_unlabeled_dataset_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd342738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b4e47",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf8080c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "sdg_params = dict(alpha=1e-5, penalty='l2', loss='log_loss')\n",
    "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab8212",
   "metadata": {},
   "source": [
    "Supervised Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb32c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(**vectorizer_params)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(**sdg_params)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad91f32",
   "metadata": {},
   "source": [
    "Self Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ed4b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelfTraining Pipeline\n",
    "st_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(**vectorizer_params)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SelfTrainingClassifier(SGDClassifier(**sdg_params), verbose=True)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa49e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\n",
    "    print(\"Number of training samples:\", X_train.shape)\n",
    "    print(\"Unlabeled samples in training set:\",\n",
    "          sum(1 for x in y_train if x == -1))\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Micro-averaged F1 score on test set: \"\n",
    "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
    "    print(\"-\" * 10)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66814b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive' 'negative' -1]\n",
      "SelfTrainingClassifier on all of training data including unlabeled data\n",
      "Number of training samples: 74949\n",
      "Unlabeled samples in training set: 49965\n",
      "End of iteration 1, added 35347 new labels.\n",
      "End of iteration 2, added 2960 new labels.\n",
      "End of iteration 3, added 597 new labels.\n",
      "End of iteration 4, added 180 new labels.\n",
      "End of iteration 5, added 72 new labels.\n",
      "End of iteration 6, added 24 new labels.\n",
      "End of iteration 7, added 79 new labels.\n",
      "End of iteration 8, added 16 new labels.\n",
      "End of iteration 9, added 8 new labels.\n",
      "End of iteration 10, added 3 new labels.\n",
      "Micro-averaged F1 score on test set: 0.891\n",
      "----------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.88      0.89     12492\n",
      "    positive       0.89      0.90      0.89     12493\n",
      "\n",
      "    accuracy                           0.89     24985\n",
      "   macro avg       0.89      0.89      0.89     24985\n",
      "weighted avg       0.89      0.89      0.89     24985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with unlabeled data\n",
    "if __name__ == \"__main__\":\n",
    "    X_train, y_train = data_train_unlabeled.Text, data_train_unlabeled.Sentiment\n",
    "    X_test, y_test = data_test.Text, data_test.Sentiment\n",
    "    \n",
    "    #making the '-1' string label to integer\n",
    "    y_train[y_train == '-1'] = -1\n",
    "    print(y_train.unique())\n",
    "    \n",
    "    print(\"SelfTrainingClassifier on all of training data including unlabeled data\")\n",
    "    eval_and_print_metrics(st_pipeline, X_train, y_train, X_test, y_test)\n",
    "    print(classification_report(y_test, st_pipeline.predict(X_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ac131f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive' 'negative']\n",
      "SelfTrainingClassifier on all of training data including unlabeled data\n",
      "Number of training samples: 24984\n",
      "Unlabeled samples in training set: 0\n",
      "Micro-averaged F1 score on test set: 0.902\n",
      "----------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.89      0.90     12492\n",
      "    positive       0.89      0.91      0.90     12493\n",
      "\n",
      "    accuracy                           0.90     24985\n",
      "   macro avg       0.90      0.90      0.90     24985\n",
      "weighted avg       0.90      0.90      0.90     24985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#without unlabeled data\n",
    "if __name__ == \"__main__\":\n",
    "    X_train, y_train = data_train.Text, data_train.Sentiment\n",
    "    X_test, y_test = data_test.Text, data_test.Sentiment\n",
    "    \n",
    "    #making the '-1' string label to integer\n",
    "    #y_train[y_train == '-1'] = -1\n",
    "    print(y_train.unique())\n",
    "    \n",
    "    print(\"SelfTrainingClassifier on all of training data including unlabeled data\")\n",
    "    eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
    "    print(classification_report(y_test, pipeline.predict(X_test)))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc42506",
   "metadata": {},
   "source": [
    "Semi Supervised SGD and regular SGD Training Review Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aadf910",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_unlabeled_vals =pd.read_csv(\"training_unlabeled_vals.csv\")\n",
    "data_train_vals = pd.read_csv(\"training_vals.csv\")\n",
    "data_test_vals = pd.read_csv(\"test_vals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6092077e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaush\\AppData\\Local\\Temp\\ipykernel_9828\\1349929074.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train_unlabeled[y_train_unlabeled == '-1'] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9  8 10  7  3  4  1  2 -1]\n",
      "SelfTrainingClassifier on all of training data including unlabeled data\n",
      "Number of training samples: 74949\n",
      "Unlabeled samples in training set: 49965\n",
      "End of iteration 1, added 1338 new labels.\n",
      "End of iteration 2, added 666 new labels.\n",
      "End of iteration 3, added 445 new labels.\n",
      "End of iteration 4, added 321 new labels.\n",
      "End of iteration 5, added 132 new labels.\n",
      "End of iteration 6, added 3 new labels.\n",
      "End of iteration 7, added 5 new labels.\n",
      "End of iteration 8, added 28 new labels.\n",
      "End of iteration 9, added 76 new labels.\n",
      "End of iteration 10, added 30 new labels.\n",
      "Micro-averaged F1 score on test set: 0.440\n",
      "----------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.86      0.65      5018\n",
      "           2       0.25      0.04      0.07      2300\n",
      "           3       0.30      0.16      0.21      2541\n",
      "           4       0.33      0.33      0.33      2633\n",
      "           7       0.34      0.24      0.28      2305\n",
      "           8       0.28      0.21      0.24      2848\n",
      "           9       0.21      0.04      0.06      2344\n",
      "          10       0.49      0.81      0.61      4996\n",
      "\n",
      "    accuracy                           0.44     24985\n",
      "   macro avg       0.34      0.34      0.31     24985\n",
      "weighted avg       0.38      0.44      0.38     24985\n",
      "\n",
      "Supervised SGDClassifier on training data not including unlabeled data\n",
      "Number of training samples: 24984\n",
      "Unlabeled samples in training set: 0\n",
      "Micro-averaged F1 score on test set: 0.436\n",
      "----------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.84      0.65      5018\n",
      "           2       0.24      0.07      0.11      2300\n",
      "           3       0.30      0.16      0.20      2541\n",
      "           4       0.33      0.33      0.33      2633\n",
      "           7       0.33      0.25      0.29      2305\n",
      "           8       0.28      0.26      0.27      2848\n",
      "           9       0.21      0.05      0.08      2344\n",
      "          10       0.51      0.76      0.61      4996\n",
      "\n",
      "    accuracy                           0.44     24985\n",
      "   macro avg       0.34      0.34      0.32     24985\n",
      "weighted avg       0.38      0.44      0.38     24985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #unlabeled with vals\n",
    "    \n",
    "    X_train_unlabeled, y_train_unlabeled = data_train_unlabeled_vals.Text, data_train_unlabeled_vals.Sentiment\n",
    "    X_test, y_test = data_test_vals.Text, data_test_vals.Sentiment\n",
    "    \n",
    "    #making the '-1' string label to integer\n",
    "    y_train_unlabeled[y_train_unlabeled == '-1'] = -1\n",
    "    print(y_train_unlabeled.unique())\n",
    "    \n",
    "    print(\"SelfTrainingClassifier on all of training data including unlabeled data\")\n",
    "    eval_and_print_metrics(st_pipeline, X_train_unlabeled, y_train_unlabeled, X_test, y_test)\n",
    "    print(classification_report(y_test, st_pipeline.predict(X_test)))\n",
    "    \n",
    "    #without unlabeled with vals\n",
    "    X_train_vals, y_train_vals = data_train_vals.Text, data_train_vals.Sentiment\n",
    "    print(\"Supervised SGDClassifier on training data not including unlabeled data\")\n",
    "    eval_and_print_metrics(pipeline, X_train_vals, y_train_vals, X_test, y_test)\n",
    "    print(classification_report(y_test, pipeline.predict(X_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b87881",
   "metadata": {},
   "source": [
    "Semi Supervised Support Vector Machines(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe63928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM classifier\n",
    "train_set = pd.read_csv(\"training_unlabeled_vals.csv\")\n",
    "test_set = pd.read_csv(\"test_vals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c78c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a19e692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99934 entries, 0 to 99933\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   index      99934 non-null  int64 \n",
      " 1   Text       99934 non-null  object\n",
      " 2   Sentiment  99934 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b4dd82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               Text  Sentiment\n",
       "0      0  Bromwell High is a cartoon comedy. It ran at t...          9\n",
       "1      1  Homelessness (or Houselessness as George Carli...          8\n",
       "2      2  Brilliant over-acting by Lesley Ann Warren. Be...         10\n",
       "3      3  This is easily the most underrated film inn th...          7\n",
       "4      4  This is not the typical Mel Brooks film. It wa...          8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78231960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d507ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedData = [] \n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "swords = stopwords.words(\"english\")\n",
    "for text in data[\"Text\"]:\n",
    "    \n",
    "    #clean links \n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    #clean everything but the alphabetical and numerical characters\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\",\" \",text)\n",
    "    \n",
    "    #Tokenizing and lemmatizing - getting words in their base form\n",
    "    text = nltk.word_tokenize(text.lower())\n",
    "    text = [lemma.lemmatize(word) for word in text]\n",
    "    \n",
    "    #Removing the stopwords\n",
    "    text = [word for word in text if word not in swords]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    cleanedData.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d09983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high cartoon comedy ran time program school life teacher 35 year teaching profession lead believe bromwell high satire much closer reality teacher scramble survive financially insightful student see right pathetic teacher pomp pettiness whole situation remind school knew student saw episode student repeatedly tried burn school immediately recalled high classic line inspector sack one teacher student welcome bromwell high expect many adult age think bromwell high far fetched pity\n",
      "\n",
      "homelessness houselessness george carlin stated ha issue year never plan help street considered human everything going school work vote matter people think homeless lost cause worrying thing racism war iraq pressuring kid succeed technology election inflation worrying next end street br br given bet live street month without luxury home entertainment set bathroom picture wall computer everything treasure see like homeless goddard bolt lesson br br mel brook directs star bolt play rich man ha everything world deciding make bet sissy rival jeffery tambor see live street thirty day without luxury bolt succeeds want future project making building bet bolt thrown street bracelet leg monitor every move step sidewalk given nickname pepto vagrant written forehead bolt meet character including woman name molly lesley ann warren ex dancer got divorce losing home pal sailor howard morris fume teddy wilson already used street survivor bolt used reaching mutual agreement like rich fight flight kill killed br br love connection molly bolt necessary plot found life stink one mel brook observant film prior comedy show tender side compared slapstick work blazing saddle young frankenstein spaceballs matter show like something valuable losing next day hand making stupid bet like rich people know money maybe give homeless instead using like monopoly money br br maybe film inspire help others\n",
      "\n",
      "brilliant acting lesley ann warren best dramatic hobo lady ever seen love scene clothes warehouse second none corn face classic good anything blazing saddle take lawyer also superb accused turncoat selling bos dishonest lawyer pepto bolt shrug indifferently lawyer say three funny word jeffrey tambor favorite later larry sander show fantastic mad millionaire want crush ghetto character malevolent usual hospital scene scene homeless invade demolition site time classic look leg scene two big digger fighting one bleeds movie get better time see quite often\n",
      "\n",
      "easily underrated film inn brook cannon sure flawed doe give realistic view homelessness unlike say citizen kane gave realistic view lounge singer titanic gave realistic view italian idiot many joke fall flat still film lovable way many comedy pull story traditionally reviled member society truly impressive fisher king crap either complaint brook cast someone else lead love mel director writer much lead\n",
      "\n",
      "typical mel brook film wa much le slapstick movie actually plot wa followable leslie ann warren made movie fantastic rated actress moment could fleshed bit scene could probably cut make room worth price rent see acting wa good overall brook good job without characteristic speaking directly audience warren wa best actor movie fume sailor played part well\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(cleanedData[i],end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e6a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the bag of words\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "BOW = vectorizer.fit_transform(cleanedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17c56911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(BOW,np.asarray(data[\"Sentiment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6487b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74950, 1000)\n",
      "(24984, 1000)\n",
      "(74950,)\n",
      "(24984,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44f6a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline with svc classifier\n",
    "pipeline_svc = Pipeline([\n",
    "    ('vect', CountVectorizer(**vectorizer_params)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "# SelfTraining Pipeline\n",
    "st_pipeline_svc = Pipeline([\n",
    "    ('vect', CountVectorizer(**vectorizer_params)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SelfTrainingClassifier(SVC(probability = True), verbose=True)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04ea5d62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfTrainingClassifier SVC on all of training data including unlabeled data\n",
      "Number of training samples: (74950, 100)\n",
      "Unlabeled samples in training set: 37555\n",
      "End of iteration 1, added 3 new labels.\n",
      "Micro-averaged F1 score on test set: 0.171\n",
      "----------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00     12410\n",
      "           1       0.20      0.74      0.31      2560\n",
      "           2       0.06      0.00      0.00      1129\n",
      "           3       0.10      0.05      0.07      1270\n",
      "           4       0.12      0.17      0.14      1330\n",
      "           7       0.12      0.10      0.11      1224\n",
      "           8       0.12      0.13      0.13      1491\n",
      "           9       0.05      0.00      0.00      1123\n",
      "          10       0.17      0.73      0.28      2447\n",
      "\n",
      "    accuracy                           0.17     24984\n",
      "   macro avg       0.10      0.21      0.12     24984\n",
      "weighted avg       0.07      0.17      0.08     24984\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaush\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kaush\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kaush\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#training with svc pipeline 100 features\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"SelfTrainingClassifier SVC on all of training data including unlabeled data\")\n",
    "    #print(x_train[:500])\n",
    "    eval_and_print_metrics(model, x_train, y_train, x_test, y_test)\n",
    "    print(classification_report(y_test, model.predict(x_test)))\n",
    "    \n",
    "    #without unlabeled with vals\n",
    "    '''X_train_vals, y_train_vals = data_train_vals.Text, data_train_vals.Sentiment\n",
    "    print(\"Supervised SGDClassifier on training data not including unlabeled data\")\n",
    "    eval_and_print_metrics(pipeline, X_train_vals, y_train_vals, X_test, y_test)\n",
    "    print(classification_report(y_test, pipeline.predict(X_test)))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45f53dfb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfTrainingClassifier SVC on all of training data including unlabeled data\n",
      "Number of training samples: (74950, 1000)\n",
      "Unlabeled samples in training set: 37422\n",
      "End of iteration 1, added 1084 new labels.\n",
      "End of iteration 2, added 422 new labels.\n",
      "End of iteration 3, added 161 new labels.\n",
      "End of iteration 4, added 44 new labels.\n",
      "End of iteration 5, added 19 new labels.\n",
      "End of iteration 6, added 3 new labels.\n",
      "End of iteration 7, added 1 new labels.\n",
      "Micro-averaged F1 score on test set: 0.203\n",
      "----------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00     12543\n",
      "           1       0.24      0.83      0.37      2459\n",
      "           2       0.11      0.01      0.02      1147\n",
      "           3       0.13      0.11      0.12      1243\n",
      "           4       0.17      0.32      0.22      1343\n",
      "           7       0.15      0.16      0.16      1241\n",
      "           8       0.12      0.20      0.15      1455\n",
      "           9       0.08      0.00      0.01      1128\n",
      "          10       0.22      0.80      0.35      2425\n",
      "\n",
      "    accuracy                           0.20     24984\n",
      "   macro avg       0.14      0.27      0.16     24984\n",
      "weighted avg       0.08      0.20      0.11     24984\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaush\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kaush\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kaush\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#training with svc pipeline \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"SelfTrainingClassifier SVC on all of training data including unlabeled data\")\n",
    "    #print(x_train[:500])\n",
    "    eval_and_print_metrics(model, x_train, y_train, x_test, y_test)\n",
    "    print(classification_report(y_test, model.predict(x_test)))\n",
    "    \n",
    "    #without unlabeled with vals\n",
    "    '''X_train_vals, y_train_vals = data_train_vals.Text, data_train_vals.Sentiment\n",
    "    print(\"Supervised SGDClassifier on training data not including unlabeled data\")\n",
    "    eval_and_print_metrics(pipeline, X_train_vals, y_train_vals, X_test, y_test)\n",
    "    print(classification_report(y_test, pipeline.predict(X_test)))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34acad0e",
   "metadata": {},
   "source": [
    "Training with regular SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training with regular SVM\n",
    "train_set2 = pd.read_csv(\"training_vals.csv\")\n",
    "test_set2 = pd.read_csv(\"test_vals.csv\")\n",
    "data2 = pd.concat([train_set, test_set])\n",
    "data2.reset_index(inplace=True)\n",
    "\n",
    "cleanedData2 = [] \n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "swords = stopwords.words(\"english\")\n",
    "for text in data2[\"Text\"]:\n",
    "    \n",
    "    #clean links \n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    #clean everything but the alphabetical and numerical characters\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\",\" \",text)\n",
    "    \n",
    "    #Tokenizing and lemmatizing - getting words in their base form\n",
    "    text = nltk.word_tokenize(text.lower())\n",
    "    text = [lemma.lemmatize(word) for word in text]\n",
    "    \n",
    "    #Removing the stopwords\n",
    "    text = [word for word in text if word not in swords]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    cleanedData2.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ceab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "BOW = vectorizer.fit_transform(cleanedData2)\n",
    "\n",
    "#split data into train and test \n",
    "x_train2,x_test2,y_train2,y_test2 = train_test_split(BOW,np.asarray(data2[\"Sentiment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bcb60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svc = SVC(probability = True)\n",
    "eval_and_print_metrics(model_svc, x_train2, y_train2, x_test2, y_test2)\n",
    "print(classification_report(y_test2, model_svc.predict(x_test2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
