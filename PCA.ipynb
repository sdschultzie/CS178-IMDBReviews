{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50df3d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import zero_one_loss as J01\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import scipy\n",
    "\n",
    "import sklearn.tree as tree\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Fix the random seed for reproducibility\n",
    "# !! Important !! : do not change this\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "DATA_PATH = Path('data/original-dataset')\n",
    "VOCAB_PATH = DATA_PATH.joinpath('imdb.vocab')\n",
    "TRAIN_BOW_PATH = DATA_PATH.joinpath('train/labeledBow.feat')\n",
    "TEST_BOW_PATH = DATA_PATH.joinpath('test/labeledBow.feat')\n",
    "COLORMAP = 'seismic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77bfefbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the' 'and' 'a' 'of' 'to' 'is' 'it' 'in' 'i' 'this']\n"
     ]
    }
   ],
   "source": [
    "with open(VOCAB_PATH, 'r', encoding='utf8') as vocab_file:\n",
    "    VOCAB = np.array(vocab_file.read().split('\\n'))\n",
    "    \n",
    "print(VOCAB[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a82c5a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read BOW file into X, Y\n",
    "def get_data_from_bow(bow_file_path):\n",
    "    \"\"\"\n",
    "    Returns a tuple (X, y):\n",
    "        X is a sparse document term matrix of size m reviews by n terms\n",
    "        y is a numpy array of size mx1 of review labels\n",
    "    \"\"\"\n",
    "    # to build sparse matrix X\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    # to build labels y\n",
    "    y = []\n",
    "\n",
    "    with open(bow_file_path, 'r', encoding='utf8') as train_bow_file:    \n",
    "        for (doc_idx, line) in enumerate(train_bow_file):\n",
    "            review = line.split(' ')\n",
    "            rating = int(review[0])\n",
    "            y.append(1 if rating > 5 else -1)\n",
    "\n",
    "            for bow_count in review[1:]:\n",
    "                vocab_idx, count = bow_count.split(':')\n",
    "                vocab_idx = int(vocab_idx)\n",
    "                count = int(count)\n",
    "                row.append(doc_idx)\n",
    "                col.append(vocab_idx)\n",
    "                data.append(count)\n",
    "\n",
    "    X = scipy.sparse.csr_matrix((data, (row, col)), dtype=np.float32)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47acd4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 89527)\n",
      "[ 1.0083544  1.0340933  1.0336381 ... 10.433524  10.433524  10.433524 ]\n",
      "89527\n",
      "[55.329884  14.397222  10.538151  10.043274   9.404477   8.429554\n",
      "  8.235899   8.091376   7.6170015  7.269477 ]\n",
      "[[ 5.0814509e-01  2.4986804e-01  2.5067049e-01 ...  6.1058035e-06\n",
      "   1.7402563e-05  1.1357053e-05]\n",
      " [-3.3597150e-01 -8.2165316e-02 -8.0620795e-02 ... -3.0703446e-05\n",
      "  -2.8154551e-05 -1.8037717e-05]\n",
      " [-1.2522189e-01  6.9059923e-02  8.6233623e-02 ...  3.6079331e-05\n",
      "  -2.9601859e-05 -1.0663361e-05]\n",
      " ...\n",
      " [-6.9671266e-02  5.9843354e-02  1.8856047e-01 ... -2.8348524e-05\n",
      "   5.9734521e-05  1.8816822e-06]\n",
      " [-2.6120454e-02 -3.6962113e-01  2.4091801e-01 ... -6.2934703e-07\n",
      "  -5.8884074e-05 -8.2857459e-06]\n",
      " [ 1.0918891e-01 -2.1676689e-01 -3.9587843e-01 ...  1.2467788e-05\n",
      "   6.7808309e-05  2.4440062e-05]]\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "Xtr, Ytr = get_data_from_bow(TRAIN_BOW_PATH)\n",
    "print(Xtr.shape)\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svd', TruncatedSVD(n_components=10, algorithm='arpack'))\n",
    "    # add learners here to the pipeline here. Don't call .fit() on test data\n",
    "]).fit(Xtr, Ytr)\n",
    "\n",
    "print(pipe['tfidf'].idf_)\n",
    "print(pipe['tfidf'].n_features_in_)\n",
    "print(pipe['svd'].singular_values_)\n",
    "print(pipe['svd'].components_)\n",
    "transform = pipe.transform(X)\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cd87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Analysis\n",
    "print(transform.shape)\n",
    "print(transform)\n",
    "\n",
    "svd_components = pipe['svd'].components_\n",
    "\n",
    "labeled_pcs = []\n",
    "for (i, component) in enumerate(svd_components):\n",
    "    terms = [(val, VOCAB[i]) for (i, val) in enumerate(component)]\n",
    "    terms.sort(reverse=True)\n",
    "    print(f'PRINCIPLE COMPONENT {i}')\n",
    "    print('Most important terms')\n",
    "    print(terms[:10])\n",
    "    print()\n",
    "    print('Least important terms')\n",
    "    print(terms[-10:])\n",
    "\n",
    "# # first = np.array(X[:10])\n",
    "# # second = np.array(X[-10:])\n",
    "\n",
    "# # print(first)\n",
    "# # print('-------------------------------')\n",
    "# # print(second)\n",
    "\n",
    "# # Xsmall = np.concatenate((first, second))[:,:50]\n",
    "# # Xsmall = Xsmall / Xsmall.sum(axis=1,keepdims=1)\n",
    "\n",
    "# k = 2\n",
    "# U, S, Vh = scipy.sparse.linalg.svds(Xsparse, k=k) # X0 = U * diag(S) * Vh\n",
    "# S = np.diag(S)\n",
    "\n",
    "# print(U)\n",
    "# print('--------')\n",
    "# print(S)\n",
    "# print('--------')\n",
    "# print(Vh)\n",
    "\n",
    "# Usmall = np.delete(U, np.s_[30:-30], axis=0)\n",
    "# Vhsmall = np.delete(Vh, np.s_[50:-50], axis=1)\n",
    "\n",
    "# print(Usmall.shape)\n",
    "# print(Vhsmall.shape)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.imshow(Usmall.dot(S), cmap=COLORMAP)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.imshow(Vhsmall, cmap=COLORMAP)\n",
    "\n",
    "\n",
    "# Xhatsmall = Usmall.dot( S ).dot( Vhsmall ) # approx using k largest eigendir\n",
    "# print(Xhatsmall.shape)\n",
    "# # Xsmall = first_and_last(X, 20)\n",
    "# # print(Xsmall.shape)\n",
    "# # Xhat_small = first_and_last(Xhat, 20)\n",
    "# # print(Xsmall.shape)\n",
    "\n",
    "# Xsmall = np.delete(np.delete(X, np.s_[30:-30], axis=0), np.s_[50:-50], axis=1)\n",
    "# print(Xsmall.shape)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# im = ax.imshow(Xsmall, cmap=COLORMAP, vmax=0.1, vmin=-0.1)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# im = ax.imshow(Xhatsmall, cmap=COLORMAP, vmax=1, vmin=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d706c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs178",
   "language": "python",
   "name": "cs178"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
